{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronalds82/Datu-kopas-tulkosana/blob/main/SQuAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDb3P--Qm0Fn"
      },
      "source": [
        "Autors: *Ronalds Turnis, rt20018*\n",
        "===========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globālie mainīgie"
      ],
      "metadata": {
        "id": "AEDGkRgwVNkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INSTALL = False # True, ja videi ir nepieciešamas papildus instalācijas\n",
        "TRAIN = True  # True, ja modelim ir nepieciešama satura regulēšana\n",
        "FINE_TUNED = False # True, ja testējam pašu saglabātu modeli\n",
        "CHAT = False # True, ja vēlamies ieslēgt tērzētavu ar modeli\n",
        "FLATTEN = True # True, ja ir nepieciešama SQuAD datu kopas \"saspiešana\" (ja faili neeksistē)\n",
        "\n",
        "weights_dir = './weights' # Tiek glabāti modeļa svari\n",
        "logs_dir = './logs' # Tiek glabāta notikumu informācija\n",
        "tmp_dir = './tmp_trainer' # Glabājās pagaidu informācija\n",
        "\n",
        "# Jānorāda, ja vēlamies veikt pašu saglabāta modeļa novērtēšanu\n",
        "model_link = 'https://www.dropbox.com/scl/fi/cguiqykr0jlp1yj6mi7rm/modelis.pth?rlkey=m7oeyaev1pt0x1c5u2kdnl0bu&dl=0'\n",
        "model_weight_file = 'modelis.pth?rlkey=m7oeyaev1pt0x1c5u2kdnl0bu'\n",
        "\n",
        "# 1.1 vai 2.0\n",
        "SQuAD_version = '2.0'\n",
        "\n",
        "# SQUAD testēšanai nepieciešamie mainīgie\n",
        "data_files = {'train': f'train-v{SQuAD_version}.LV.json', 'validation': f'train-v{SQuAD_version}.LV.json'}\n",
        "flattened_train_file_1 = 'flattened_train_file_1.json'\n",
        "flattened_val_file_2 = 'flattened_val_file_2.json'\n",
        "\n",
        "# Šie ir nepieciešami failu saglabāšanai, bet netiek izmantoti\n",
        "flattened_val_file_1 = 'flattened_val_file_1.json'\n",
        "flattened_train_file_2 = 'flattened_train_file_2.json'\n",
        "\n",
        "# Faili, kuri tiks izmantoti apmācības un validācijas laikā\n",
        "flattened_data_files = {'train': flattened_train_file_1, 'validation': flattened_val_file_2}\n",
        "\n",
        "test_size = 0.2 # LV validācijas datu kopas izmērs attiecībā pret apmācības datu kopu\n",
        "\n",
        "model_name = 'bert-base-multilingual-cased' # Norādām HuggingFace modeli, kurš tiks izmantots\n",
        "\n",
        "max_length = 384 # Maksimālais garums context un question string tipa mainīgajiem\n",
        "doc_stride = 128 # Pārlaidums starp segmentiem, lai apstrādātu garākus tekstus, nekā modelis var apstrādāt vienlaikus"
      ],
      "metadata": {
        "id": "4uqoL3rjVPF3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BhXjCQDy2wU"
      },
      "source": [
        "# Moduļu importēšana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OePrTPIty3It"
      },
      "outputs": [],
      "source": [
        "# Ja ir nepieciešamas papildus instalācijas\n",
        "if INSTALL:\n",
        "    !pip install datasets\n",
        "    !pip install transformers[torch] -U\n",
        "    !pip install accelerate -U\n",
        "    !pip install rouge_score\n",
        "    !pip install evaluate\n",
        "\n",
        "# Importējam nepieciešamos moduļus\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import evaluate\n",
        "import json\n",
        "import collections\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from transformers import Trainer, TrainingArguments, IntervalStrategy, DataCollatorForLanguageModeling, trainer_utils, EvalPrediction\n",
        "from transformers import default_data_collator, AutoTokenizer, AutoModelForQuestionAnswering, tokenization_utils_base\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset, random_split\n",
        "from datasets import load_dataset, arrow_dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeļa un tokenizera inicializācija"
      ],
      "metadata": {
        "id": "EaBULUuZ9oXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ielādējam modeli un tokenizeru...\")\n",
        "\n",
        "# Modeļa un tokenizera definēšana\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Iestatām padding tokenu, ja tā nav\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Pārbaudām CUDA pieejamību un uzstādām modeli uz atbilstošās ierīces\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Modelis un tokenizeris inicializēts!\")\n",
        "print(\"Izmantotā ierīce:\", device)"
      ],
      "metadata": {
        "id": "Wd5GcD3b9qcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iepriekšējo datu dzēšana"
      ],
      "metadata": {
        "id": "BOrjb0UYXZsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pārbaudām vai eksistē ģenerētās direktorijas un dzēšam tās pirms modeļa trenēšanas, lai nekonfliktētu ar vecajiem datiem\n",
        "def delete_directory(dir_path):\n",
        "    try:\n",
        "        if os.path.exists(dir_path):\n",
        "            shutil.rmtree(dir_path)\n",
        "            print(f\"Direktorija dzēsta: {dir_path}\")\n",
        "        else:\n",
        "            print(f\"Direktorija neeksistē: {dir_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Kļūda dzēšot direktoriju: {e}\")\n",
        "\n",
        "delete_directory(weights_dir)\n",
        "delete_directory(logs_dir)\n",
        "delete_directory(tmp_dir)\n",
        "\n",
        "# Izdzēšam GPU kešatmiņu\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DfkYYPRvXb8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eirsvXFakQcV"
      },
      "source": [
        "# Datu inicializācija"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU1BJRZ_kWif"
      },
      "outputs": [],
      "source": [
        "# Pārbaudam, vai \"spilventiņš\" ir uz labās puses\n",
        "pad_on_right = tokenizer.padding_side == \"right\"\n",
        "\n",
        "# SquadDatasetFlat klase ar norādītajiem failiem un ceļiem\n",
        "class SquadDatasetFlat():\n",
        "    def __init__(self,\n",
        "                 path_to_json_file: str,\n",
        "                 checkpoint_path: str,\n",
        "                 train_file: str,\n",
        "                 val_file: str) -> None:\n",
        "        self.path_to_json_file = path_to_json_file\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "        self.train_file = train_file\n",
        "        self.val_file = val_file\n",
        "\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        # Atver un nolasa JSON failu ar apmācības datiem\n",
        "        with open(self.path_to_json_file, 'r') as f:\n",
        "            train_data = json.load(f)\n",
        "        print(f'Flattening SQUAD {train_data[\"version\"]}')\n",
        "\n",
        "        # Izlīdzina SQUAD datus apmācībai un validācijai\n",
        "        train_data_flat, val_data_flat, errors = self.load_squad_data(train_data)\n",
        "        print(f'\\nErroneous Datapoints: {errors}')\n",
        "\n",
        "        # Saglabā izlīdzinātos apmācības datus\n",
        "        with open(Path(self.checkpoint_path) / Path(self.train_file), 'w') as file:\n",
        "            train_data = {'data': train_data_flat}\n",
        "            file.write(json.dumps(train_data))\n",
        "            file.close()\n",
        "\n",
        "        # Saglabā izlīdzinātos validācijas datus\n",
        "        with open(Path(self.checkpoint_path) / Path(self.val_file), 'w') as file:\n",
        "            val_data = {'data': val_data_flat}\n",
        "            file.write(json.dumps(val_data))\n",
        "            file.close()\n",
        "\n",
        "    def load_squad_data(self, data):\n",
        "        # Inicializē kļūdu skaitu un tukšus sarakstus izlīdzinātiem datiem\n",
        "        errors = 0\n",
        "        flattened_data_train = []\n",
        "        flattened_data_val = []\n",
        "\n",
        "        # Aprēķina robežu, līdz kurai dati tiks izmantoti apmācībai\n",
        "        train_range = len(data['data']) - (len(data['data']) * test_size)\n",
        "\n",
        "        # Pāriet cauri visiem datiem un izlīdzina tos\n",
        "        for i, article in enumerate(data[\"data\"]):\n",
        "            title = article.get(\"title\", \"\").strip()\n",
        "            for paragraph in article[\"paragraphs\"]:\n",
        "                context = paragraph[\"context\"].strip()\n",
        "                for qa in paragraph[\"qas\"]:\n",
        "                    question = qa[\"question\"].strip()\n",
        "                    id_ = qa[\"id\"]\n",
        "\n",
        "                    # Iegūst atbildes sākuma pozīcijas un pašas atbildes\n",
        "                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
        "                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n",
        "\n",
        "                    # Pievieno datus apmācībai vai validācijai atkarībā no indeksa\n",
        "                    if i <= train_range:\n",
        "                        flattened_data_train.append({\"title\": title,\n",
        "                                                     \"context\": context,\n",
        "                                                     \"question\": question,\n",
        "                                                     \"id\": id_,\n",
        "                                                     \"answers\": {\n",
        "                                                         \"answer_start\": answer_starts,\n",
        "                                                         \"text\": answers}\n",
        "                                                     })\n",
        "                    else:\n",
        "                        flattened_data_val.append({\"title\": title,\n",
        "                                                   \"context\": context,\n",
        "                                                   \"question\": question,\n",
        "                                                   \"id\": id_,\n",
        "                                                   \"answers\": {\n",
        "                                                       \"answer_start\": answer_starts,\n",
        "                                                       \"text\": answers}\n",
        "                                                   })\n",
        "\n",
        "        # Atgriež izlīdzinātos apmācības un validācijas datus kopā ar kļūdu skaitu\n",
        "        return flattened_data_train, flattened_data_val, errors\n",
        "\n",
        "def prepare_squad_dataset(examples: collections.OrderedDict or dict) -> tokenization_utils_base.BatchEncoding:\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Tā kā viens piemērs var mums dot vairākus paraugus, ja tajā ir garš konteksts, mums ir vajadzīga karte no parauga uz\n",
        "    # tā atbilstošo piemēru. Šī atslēga mums to dod\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Offsetu kartējumi mums dos karti no tokena uz rakstzīmes pozīciju sākotnējā kontekstā. Tas\n",
        "    # palīdzēs mums aprēķināt starta pozīcijas un beigu pozīcijas\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # CLS indeksa iegūšana\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Paņemam sekvenci, kas atbilst šim piemēram (lai zinātu, kas ir konteksts un kas ir jautājums)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Viens piemērs var dot vairākus spanus, šis ir indekss piemēram, kas satur šo tekstu\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Atbilžu sākuma/beigu rakstzīmju indeksi tekstā\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Sākuma tokena indekss pašreizējam spanam tekstā\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # Beigu tokena indekss pašreizējam spanam tekstā\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Pārbaudām, vai atbilde ir ārpus spana (šajā gadījumā šis paraugs tiek marķēts ar CLS indeksu)\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Pārvietojam token_start_index un token_end_index uz atbildes abiem galiem\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "def prepare_validation_features(examples: collections.OrderedDict or dict) -> tokenization_utils_base.BatchEncoding:\n",
        "    # Tokenizējam mūsu piemērus ar trancēšanu un varbūt arī ar spilventiņu, bet paturam pārplūdes, izmantojot stride. Tas rezultējas\n",
        "    # vienā piemēra iespējā dot vairākus paraugus, ja konteksts ir garš, katram no šiem paraugiem ir\n",
        "    # konteksts, kas pārklājas mazliet ar iepriekšējā parauga kontekstu\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Tā kā viens piemērs var mums dot vairākus paraugus, ja tajā ir garš konteksts, mums ir vajadzīga karte no parauga uz\n",
        "    # tā atbilstošo piemēru. Šī atslēga mums to dod\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Offsetu kartējumi mums dos karti no tokena uz rakstzīmes pozīciju sākotnējā kontekstā. Tas\n",
        "    # palīdzēs mums aprēķināt starta_pozīcijas un beigu_pozīcijas\n",
        "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # CLS indekss\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Iegūstam secību, kas atbilst šim piemēram (lai zinātu, kas ir konteksts un kas ir jautājums)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Viens piemērs var dot vairākus posmus, šis ir piemēra indekss, kas satur šo teksta posmu\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Atbilžu sākuma/beigu rakstzīmju indeksi tekstā\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Sākuma tokena indekss pašreizējā teksta posmā\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # Beigu tokena indekss pašreizējā teksta posmā\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Nosaka, vai atbilde ir ārpus posma (šajā gadījumā šī iezīme tiek marķēta ar CLS indeksu)\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Pretējā gadījumā pārvieto token_start_index un token_end_index uz abiem atbildes galiem\n",
        "                # Piezīme: mēs varētu pāriet aiz pēdējā ofseta, ja atbilde ir pēdējais vārds (izņēmuma gadījums)\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    # Mēs saglabājam example_id, kas mums deva šo iezīmi, un mēs saglabāsim ofsetu kartējumus\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Iegūstam secību, kas atbilst šim piemēram (lai zinātu, kas ir konteksts un kas ir jautājums)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # Viens piemērs var dot vairākus posmus, šis ir piemēra indekss, kas satur šo teksta posmu\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Nosaka par None ofsetu kartējumus, kas nav daļa no konteksta, lai būtu viegli noteikt, vai tokena\n",
        "        # pozīcija ir daļa no konteksta vai nē\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "# Sagatavojam datus\n",
        "if FLATTEN:\n",
        "    _ = SquadDatasetFlat(data_files['train'], '', flattened_train_file_1, flattened_val_file_1)\n",
        "    _ = SquadDatasetFlat(data_files['validation'], '', flattened_train_file_2, flattened_val_file_2)\n",
        "\n",
        "train_data = load_dataset(\"json\", data_files=flattened_data_files['train'], field='data')\n",
        "val_data = load_dataset(\"json\", data_files=flattened_data_files['validation'], field='data')\n",
        "\n",
        "train_dataset = train_data.map(prepare_squad_dataset, batched=True, remove_columns=train_data['train'].column_names)\n",
        "val_dataset = val_data['train'].map(prepare_validation_features, batched=True, remove_columns=val_data['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset['train'][:5])"
      ],
      "metadata": {
        "id": "bl0hxs7Daqr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQuAD datu kopas analīze"
      ],
      "metadata": {
        "id": "cayptggfYvCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(answer: list) -> str:\n",
        "  if len(answer) == 0:\n",
        "      return ''\n",
        "  else:\n",
        "      return answer[0]\n",
        "\n",
        "def get_json_data(json_path: str) -> dict:\n",
        "    f = open(json_path)\n",
        "\n",
        "    # Atgriež JSON objektu kā sarakstu\n",
        "    json_data = json.load(f)\n",
        "\n",
        "    f.close()\n",
        "    return json_data\n",
        "\n",
        "train_dataframe = pd.json_normalize(get_json_data(flattened_train_file_1), record_path='data')\n",
        "train_dataframe[\"answers.text\"] = train_dataframe[\"answers.text\"].apply(get_text)\n",
        "\n",
        "train_dataframe"
      ],
      "metadata": {
        "id": "oRlT-RwzXWQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (10,6)\n",
        "train_dataframe['context'].apply(len).plot.hist(title=\"Konteksta garuma histogramma\", bins=20, figsize=figsize, grid=True)"
      ],
      "metadata": {
        "id": "6TMA_6tTYPQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe['question'].apply(len).plot.hist(title=\"Jautājumu garuma histogramma\", bins=20, figsize=figsize, grid=True)"
      ],
      "metadata": {
        "id": "rWPkEu1tYVuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metriku aprēķināšanas funkcijas"
      ],
      "metadata": {
        "id": "Qzy9BxyZxkqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history = []\n",
        "\n",
        "metric = evaluate.load(\"squad_v2\" if SQuAD_version == '2.0' else \"squad\")\n",
        "\n",
        "def postprocess_qa_predictions(examples: arrow_dataset.Dataset,\n",
        "                               features: arrow_dataset.Dataset,\n",
        "                               raw_predictions: tuple,\n",
        "                               n_best_size: int = 20,\n",
        "                               max_answer_length: int = 50) -> collections.OrderedDict:\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    all_predictions = collections.OrderedDict()\n",
        "    all_nbest_json = collections.OrderedDict()\n",
        "\n",
        "    if SQuAD_version == '2.0':\n",
        "        scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_prediction = None\n",
        "        prelim_predictions = []\n",
        "\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
        "            # available in the current feature.\n",
        "            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            feature_null_score = start_logits[0] + end_logits[0]\n",
        "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
        "                min_null_prediction = {\n",
        "                    \"offsets\": (0, 0),\n",
        "                    \"score\": feature_null_score,\n",
        "                    \"start_logit\": start_logits[0],\n",
        "                    \"end_logit\": end_logits[0],\n",
        "                }\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or len(offset_mapping[start_index]) < 2\n",
        "                        or offset_mapping[end_index] is None\n",
        "                        or len(offset_mapping[end_index]) < 2\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
        "                    # provided).\n",
        "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
        "                        continue\n",
        "\n",
        "                    prelim_predictions.append(\n",
        "                        {\n",
        "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"start_logit\": start_logits[start_index],\n",
        "                            \"end_logit\": end_logits[end_index],\n",
        "                        }\n",
        "                    )\n",
        "        if SQuAD_version == '2.0' and min_null_prediction is not None:\n",
        "            # Add the minimum null prediction\n",
        "            prelim_predictions.append(min_null_prediction)\n",
        "            null_score = min_null_prediction[\"score\"]\n",
        "\n",
        "        # Only keep the best `n_best_size` predictions.\n",
        "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "\n",
        "        # Add back the minimum null prediction if it was removed because of its low score.\n",
        "        if (\n",
        "            SQuAD_version == '2.0'\n",
        "            and min_null_prediction is not None\n",
        "            and not any(p[\"offsets\"] == (0, 0) for p in predictions)\n",
        "        ):\n",
        "            predictions.append(min_null_prediction)\n",
        "\n",
        "        # Use the offsets to gather the answer text in the original context.\n",
        "        context = example[\"context\"]\n",
        "        for pred in predictions:\n",
        "            offsets = pred.pop(\"offsets\")\n",
        "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
        "\n",
        "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "        # failure.\n",
        "        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
        "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
        "\n",
        "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
        "        # the LogSumExp trick).\n",
        "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
        "        exp_scores = np.exp(scores - np.max(scores))\n",
        "        probs = exp_scores / exp_scores.sum()\n",
        "\n",
        "        # Include the probabilities in our predictions.\n",
        "        for prob, pred in zip(probs, predictions):\n",
        "            pred[\"probability\"] = prob\n",
        "\n",
        "        # Pick the best prediction. If the null answer is not possible, this is easy.\n",
        "        if not SQuAD_version == '2.0':\n",
        "            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
        "        else:\n",
        "            # Otherwise we first need to find the best non-empty prediction.\n",
        "            i = 0\n",
        "            while predictions[i][\"text\"] == \"\":\n",
        "                i += 1\n",
        "            best_non_null_pred = predictions[i]\n",
        "\n",
        "            # Then we compare to the null prediction using the threshold.\n",
        "            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n",
        "            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n",
        "            if score_diff > 0.2:\n",
        "                all_predictions[example[\"id\"]] = \"\"\n",
        "            else:\n",
        "                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
        "\n",
        "        # Make `predictions` JSON-serializable by casting np.float back to float.\n",
        "        all_nbest_json[example[\"id\"]] = [\n",
        "            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n",
        "            for pred in predictions\n",
        "        ]\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "def compute_metrics(pred: trainer_utils.EvalPrediction) -> dict:\n",
        "    # Lai iegūtu galīgās prognozes, mēs varam pielietot mūsu pēcapstrādes funkciju mūsu sākotnējām prognozēm\n",
        "    final_predictions = postprocess_qa_predictions(val_data['train'], val_dataset, pred.predictions)\n",
        "\n",
        "    if SQuAD_version == '2.0':\n",
        "        formatted_predictions = [{\"id\": str(k), \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "    else:\n",
        "        formatted_predictions = [{\"id\": str(k), \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "\n",
        "    # Mums tikai jānomaina prognozes un marķējumi, jo metrika sagaida saraksta sarakstu, nevis vienu lielu sarakstu\n",
        "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in val_data[\"train\"]]\n",
        "\n",
        "    # Aprēķinām rezultātu\n",
        "    results = metric.compute(predictions=formatted_predictions, references=references)\n",
        "\n",
        "    results_squad = {\n",
        "        'f1': results['f1'],\n",
        "        'exact': results['exact']\n",
        "    }\n",
        "\n",
        "    metrics_history.append(results_squad)\n",
        "\n",
        "    return results_squad"
      ],
      "metadata": {
        "id": "sR79WpeQxmo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOqilKGjwrKN"
      },
      "source": [
        "# Modeļa trenēšana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkdpMGM1wtq5"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=weights_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    eval_strategy=IntervalStrategy.STEPS,\n",
        "    eval_steps=2500,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.001,\n",
        "    learning_rate=3e-5,\n",
        "    logging_dir=logs_dir,\n",
        "    logging_steps=2500,\n",
        "    save_strategy=IntervalStrategy.EPOCH,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    label_names=[\"start_positions\", \"end_positions\"]\n",
        ")\n",
        "\n",
        "# Inicializējam traineri\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset['train'],\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=default_data_collator\n",
        ")\n",
        "\n",
        "if TRAIN:\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apmācītā modeļa analīze"
      ],
      "metadata": {
        "id": "7XOyWNkL96iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(training_history):\n",
        "    # Definējam sarakstus uzskaitei\n",
        "    steps = []\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    # Ejam cauri katram ierakstam vēsturē\n",
        "    for entry in training_history:\n",
        "        if 'loss' in entry:  # Apmācības zaudējums\n",
        "            steps.append(entry.get('step', len(steps) + 1))\n",
        "            training_losses.append(entry['loss'])\n",
        "\n",
        "        if 'eval_loss' in entry:  # Validācijas zaudējums\n",
        "            validation_losses.append(entry['eval_loss'])\n",
        "\n",
        "    # Izveidojam grafiku\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    if training_losses:\n",
        "        plt.plot(steps[:len(training_losses)], training_losses, label='Apmācības zaudējums', marker='o')\n",
        "\n",
        "    if validation_losses:\n",
        "        validation_steps = steps[:len(validation_losses)]\n",
        "        plt.plot(validation_steps, validation_losses, label='Validācijas zaudējums', marker='x', linestyle='--')\n",
        "\n",
        "    plt.xlabel('Soļu skaits')\n",
        "    plt.ylabel('Zaudējums')\n",
        "    plt.title('Apmācības un validācijas zaudējums apmācības laikā')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_metrics_history(metrics_history):\n",
        "    epochs = list(range(1, len(metrics_history)+1))\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Sagatavojam datus parādīšanai grafikā\n",
        "    exact_match_scores = [x['exact'] for x in metrics_history]\n",
        "    f1_scores = [x['f1'] for x in metrics_history]\n",
        "\n",
        "    # Izveidojam grafiku\n",
        "    plt.plot(epochs, exact_match_scores, label='Exact Match', marker='o')\n",
        "    plt.plot(epochs, f1_scores, label='F1', marker='o')\n",
        "\n",
        "    plt.xlabel('Iterācija')\n",
        "    plt.ylabel('Rezultāts')\n",
        "    plt.title('Vērtēšanas metriku rezultāti atkarībā no iterācijas')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Izpildām tikai, ja ir notikusi modeļa pielāgošana, citādi tas tiks darīts \"Neapmācīta modeļa analīze\" sadaļā\n",
        "if TRAIN:\n",
        "    plot_results(trainer.state.log_history)\n",
        "    print()\n",
        "    plot_metrics_history(metrics_history)"
      ],
      "metadata": {
        "id": "FC_4BSQz-AbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apmācītā modeļa saglabāšana"
      ],
      "metadata": {
        "id": "elgrVcValnK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "    model_weights = f'{model_name}-modelis.pth'\n",
        "    torch.save(model.state_dict(), model_weights)"
      ],
      "metadata": {
        "id": "hby7YW95lpnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neapmācīta modeļa analīze\n",
        "\n"
      ],
      "metadata": {
        "id": "i1vjoQ6Kl02i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if FINE_TUNED:\n",
        "    !wget model_link\n",
        "\n",
        "    # Ielādējam saglabātos svarus\n",
        "    model.load_state_dict(torch.load(model_weight_file))\n",
        "\n",
        "    # Pārbaudām CUDA pieejamību un uzstādām modeli uz atbilstošās ierīces\n",
        "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "if TRAIN == False:\n",
        "    trainer.evaluate(val_dataset)\n",
        "    print(f'Results: {metrics_history}')"
      ],
      "metadata": {
        "id": "xQzYtLuMl42i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Čatbota\" saskarne"
      ],
      "metadata": {
        "id": "-YGopo-Qdn6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, input_text, max_length=150):\n",
        "    # Tokenizējam ievadīto tekstu\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Ģenerējam uzmanības masku\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    # Modelis ģenerē tekstu\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length + len(input_ids[0]),\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=0.9,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Dekodējam tekstu un atgriežam to\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def chat_with_model(model, tokenizer):\n",
        "    print(\"Sāciet tērzēšanu ar modeli (ierakstiet 'q', lai izietu):\")\n",
        "\n",
        "    while True:\n",
        "        input_text = input(\"You: \")\n",
        "\n",
        "        if input_text.lower() == 'q':\n",
        "            break\n",
        "\n",
        "        response = generate_text(model, tokenizer, input_text)\n",
        "        print(\"Bot:\", response)\n",
        "\n",
        "if CHAT:\n",
        "    chat_with_model(model, tokenizer)"
      ],
      "metadata": {
        "id": "Pb45i6tMdpwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}